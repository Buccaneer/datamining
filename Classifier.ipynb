{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c56a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347cc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data, removing redundant columns\n",
    "\n",
    "data = pd.read_csv('existing-customers.csv', sep=';', na_values=[''])\n",
    "y = data[['class']]\n",
    "y = pd.get_dummies(y, drop_first=True)\n",
    "y = y['class_>50K']\n",
    "data = data.drop(columns = ['RowID', 'education-num'])\n",
    "data = data.drop(columns = ['class'])\n",
    "\n",
    "new_data = pd.read_csv('potential-customers.csv', sep=';', na_values=[''])\n",
    "new_data_ID = new_data['RowID']\n",
    "new_data = new_data.drop(columns = ['RowID', 'education-num'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4a727",
   "metadata": {},
   "source": [
    "## Bayesian classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b492be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data to fit for Bayesian classifier\n",
    "X_bayes = data\n",
    "X_bayes_new = new_data\n",
    "\n",
    "# make dictionary to transform categorical data to numeric, to make sure new customer data has same numbering as training data\n",
    "cat_columns = X_bayes.select_dtypes(['object']).columns\n",
    "dictionaries = {}\n",
    "for column in cat_columns:\n",
    "    dictionary = {}\n",
    "    count = 0\n",
    "    for unique_value in X_bayes[column].unique():\n",
    "        dictionary[unique_value] = count\n",
    "        count += 1    \n",
    "    dictionaries[column] = dictionary\n",
    "    \n",
    "# transform categorical data to numeric    \n",
    "for column in cat_columns:\n",
    "    X_bayes[column] = X_bayes[column].map(dictionaries[column])\n",
    "    X_bayes_new[column] = X_bayes_new[column].map(dictionaries[column])\n",
    "X_bayes = pd.get_dummies(X_bayes, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35605f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test sets\n",
    "X_bayes_train, X_bayes_val_test, y_bayes_train, y_bayes_val_test = train_test_split(X_bayes, y, test_size=0.3)\n",
    "X_bayes_val, X_bayes_test, y_bayes_val, y_bayes_test = train_test_split(X_bayes_val_test, y_bayes_val_test, test_size=0.5)\n",
    "\n",
    "# Split the train data into multiple subsets using bootstrap sampling\n",
    "n_subsets = 10\n",
    "customer_subsets_bayes = []\n",
    "for i in range(n_subsets):\n",
    "    subset_indices = np.random.choice(len(X_bayes_train), size=len(X_bayes_train), replace=True)\n",
    "    X_bayes_train_subset = X_bayes_train.iloc[subset_indices]\n",
    "    y_bayes_train_subset = y_bayes_train.iloc[subset_indices]\n",
    "    customer_subsets_bayes.append((X_bayes_train_subset, y_bayes_train_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1420e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions to build bayes, test hyperparameters (to be used in next step)                               \n",
    "\n",
    "def build_bayes(alpha, validation):\n",
    "    # select a subset of the data for this bayes\n",
    "    subset = np.random.choice(len(customer_subsets_bayes), size=1)[0]\n",
    "    X_bayes_train_subset, y_bayes_train_subset = customer_subsets_bayes[subset]\n",
    "    # train bayes\n",
    "    bayes = CategoricalNB(force_alpha=True, alpha=alpha)\n",
    "    bayes.fit(X_bayes_train_subset, y_bayes_train_subset)\n",
    "    if validation:\n",
    "        # predict val with this bayes\n",
    "        val_predict = bayes.predict(X_bayes_val)\n",
    "        val_acc = accuracy_score(y_bayes_val, val_predict)\n",
    "        # return accuracy\n",
    "        return val_acc\n",
    "    else:\n",
    "        #return bayes\n",
    "        return bayes\n",
    "\n",
    "\n",
    "def param_testing(alphas):\n",
    "    best_acc = 0\n",
    "    for alpha in alphas:\n",
    "        val_acc = build_bayes(alpha, True) \n",
    "        if val_acc > best_acc:\n",
    "            best_alpha = alpha\n",
    "            best_acc = val_acc\n",
    "            print(f\"current best alpha: {best_alpha}, accuracy is {best_acc}\")\n",
    "    print(f\"\\n best alpha is: {best_alpha}, accuracy is {best_acc}\")\n",
    "    return best_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b854a950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best alpha: 0.0001, accuracy is 0.8404995904995906\n",
      "current best alpha: 0.1, accuracy is 0.8511466011466011\n",
      "current best alpha: 0.5, accuracy is 0.8587223587223587\n",
      "current best alpha: 1, accuracy is 0.8609746109746109\n",
      "\n",
      " best alpha is: 1, accuracy is 0.8609746109746109\n"
     ]
    }
   ],
   "source": [
    "# testing which hyperparameters give best results (can be skipped: use best parameters given in next step)\n",
    "alphas = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 10, 100, 1000, 10000] #alpha can't be zero or very close to zero\n",
    "\n",
    "best_alpha = param_testing(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9469973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building bayes with the best found alpha\n",
    "bayes = build_bayes(best_alpha, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dd46757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy of Bayesian classifier is 0.8554759467758444\n"
     ]
    }
   ],
   "source": [
    "# testing the acuracy of the selected bayes on test set\n",
    "test_predict = bayes.predict(X_bayes_test)\n",
    "test_acc = accuracy_score(y_bayes_test, test_predict)\n",
    "\n",
    "print(f\" accuracy of Bayesian classifier is {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e51562",
   "metadata": {},
   "source": [
    "## Decision tree and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da23f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming data to fit for decision tree classifier\n",
    "X_tree = data\n",
    "X_tree_new = new_data\n",
    "\n",
    "# trasforming categorical data into dummies (seperate variabe for NA)\n",
    "X_tree = pd.get_dummies(X_tree, dummy_na=True)\n",
    "X_tree_new = pd.get_dummies(X_tree_new, dummy_na=True)\n",
    "\n",
    "# checking all columns unique values, to make sure new customer data has same columns as training data\n",
    "for category in X_tree.columns.values.tolist():\n",
    "    if category not in X_tree_new.columns:\n",
    "        X_tree_new[category] = 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "022cdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, and test sets\n",
    "X_tree_train, X_tree_val_test, y_tree_train, y_tree_val_test = train_test_split(X_tree, y, test_size=0.3)\n",
    "X_tree_val, X_tree_test, y_tree_val, y_tree_test = train_test_split(X_tree_val_test, y_tree_val_test, test_size=0.5)\n",
    "\n",
    "# Split the train data into multiple subsets using bootstrap sampling\n",
    "n_subsets = 20\n",
    "customer_subsets_tree = []\n",
    "for i in range(n_subsets):\n",
    "    subset_indices = np.random.choice(len(X_tree_train), size=len(X_tree_train), replace=True)\n",
    "    X_tree_train_subset = X_tree_train.iloc[subset_indices]\n",
    "    y_tree_train_subset = y_tree_train.iloc[subset_indices]\n",
    "    customer_subsets_tree.append((X_tree_train_subset, y_tree_train_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76fded8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions to build tree, build forest, test hy_treeperparameters (to be used in next step)\n",
    "\n",
    "# Group the dummies from each categorical variable together (to be able to select features keeping dummies together)\n",
    "vars = ['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country_tree']\n",
    "groups = []\n",
    "for var in vars:\n",
    "    var_dummies = [col for col in X_tree.columns if col.startswith(var)]\n",
    "    groups.append(var_dummies)\n",
    "\n",
    "# build a decision tree with given parameters, and predict the outcome for the validation set\n",
    "def build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, validation):\n",
    "    # randomly_tree select a subset of features for tree\n",
    "    selected_groups = []\n",
    "    groups = random.sample(groups, k=n_feature)\n",
    "    selected_groups.append(groups)\n",
    "    selected_features = []\n",
    "    for groups in selected_groups:\n",
    "        for group in groups:\n",
    "            selected_features += group\n",
    "    # select a subset of the data for tree\n",
    "    subset = np.random.choice(len(customer_subsets_tree), size=1)[0]\n",
    "    X_tree_train_subset, y_tree_train_subset = customer_subsets_tree[subset]\n",
    "    X_tree_train_subset = X_tree_train_subset[selected_features]\n",
    "    # train tree\n",
    "    tree = DecisionTreeClassifier(criterion=criterion, min_samples_leaf=min_samples_leaf, max_depth=max_depth,  min_samples_split= min_samples_split)\n",
    "    tree.fit(X_tree_train_subset, y_tree_train_subset)\n",
    "    if validation:\n",
    "        # transforming validation set to fit with selected features of tree\n",
    "        X_tree_val_subset = X_tree_val[selected_features]\n",
    "        # predict val with this tree\n",
    "        predict = tree.predict(X_tree_val_subset)\n",
    "        return predict\n",
    "    else:\n",
    "        #return tree with its selected features\n",
    "        return [tree, selected_features]\n",
    "\n",
    "# build a random forest with given parameters, and calculate its accuracy_tree\n",
    "def build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, validation):\n",
    "    if validation:\n",
    "        val_predict = np.zeros((len(X_tree_val), n_tree))\n",
    "        for i in range(n_tree):\n",
    "            #fill column of validation prediction for this tree\n",
    "            val_predict[:, i] = build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, True)\n",
    "        # Aggregate the predict of all the decision trees on the val set\n",
    "        val_predict = pd.DataFrame(val_predict)\n",
    "        val_predict = val_predict.mode(axis=1)[0]\n",
    "        val_acc = accuracy_tree_score(y_tree_val, val_predict)\n",
    "        return(val_acc)\n",
    "    else:\n",
    "        forest=[]\n",
    "        features=[]\n",
    "        for i in range(n_tree):\n",
    "            #fill column of validation prediction for this tree\n",
    "            tree, selected_features = build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, False)\n",
    "            forest.append(tree) \n",
    "            features.append(selected_features)\n",
    "        return([forest, features])\n",
    "  \n",
    "    \n",
    "# test which hy_treeperparameters are best on validation set                                \n",
    "def param_testing(n_features, criterions, min_samples_leafs, max_depths, min_samples_splits, n_trees, groups):\n",
    "    best_acc = 0\n",
    "    for n_feature in n_features:   \n",
    "        for criterion in criterions: #[\"gini\", \"entropy_tree\", \"log_loss\"]\n",
    "            for min_samples_leaf in min_samples_leafs:\n",
    "                for max_depth in max_depths:\n",
    "                    for min_samples_split in min_samples_splits:\n",
    "                        for n_tree in n_trees:\n",
    "                            val_acc = build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, True) \n",
    "                            if val_acc > best_acc:\n",
    "                                best_params = [n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree]\n",
    "                                best_acc = val_acc\n",
    "                                print(f\"current best parameters are n_features: {best_params[0]}, criterion: {best_params[1]}, min_samples_leaf: {best_params[2]}, max_depth: {best_params[3]}, min_samples_split: {best_params[4]}, n_trees: {best_params[5]}, accuracy_tree is {best_acc}\")\n",
    "    print(f\"\\n best parameters are n_features: {best_params[0]}, criterion: {best_params[1]}, min_samples_leaf: {best_params[2]}, max_depth: {best_params[3]}, min_samples_split: {best_params[4]}, n_trees: {best_params[5]}, accuracy_tree is {best_acc}\")\n",
    "    return(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7538e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing which hyperparameters give best results (can be skipped: use best parameters given in next step)\n",
    "n_features = [6, 9, 12]\n",
    "criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "min_samples_leafs = [10, 20, 30]\n",
    "max_depths = [10, 20]\n",
    "min_samples_splits = [2]\n",
    "n_trees = [5, 10, 50]\n",
    "\n",
    "best_params = param_testing(n_features, criterions, min_samples_leafs, max_depths, min_samples_splits, n_trees, groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "590683a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last found best parameters (to not have to run the testing of hyperparameters again)\n",
    "# best_params = [9, \"entropy\", 10, 20, 2, 50]\n",
    "# accuracy is 0.8634316134316135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb17427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a forest with the best found parameters\n",
    "n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree = best_params\n",
    "forest, features = build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c42b9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " accuracy of random forest classifier is 0.8624360286591607\n"
     ]
    }
   ],
   "source": [
    "# testing the acuracy of the selected random forest on test set\n",
    "\n",
    "test_predict = np.zeros((len(X_tree_test), n_tree))\n",
    "for i in range(n_tree):\n",
    "    test_tree = forest[i]\n",
    "    test_features = features[i]\n",
    "    # transforming test set to fit with selected features of tree\n",
    "    X_tree_test_subset = X_tree_test[test_features]\n",
    "    # predict test with this tree\n",
    "    test_predict[:, i] = test_tree.predict(X_tree_test_subset)\n",
    "# Aggregate the predict of all the decision trees on the val set\n",
    "test_predict = pd.DataFrame(test_predict)\n",
    "test_predict = test_predict.mode(axis=1)[0]\n",
    "test_acc = accuracy_score(y_tree_test, test_predict)\n",
    "\n",
    "print(f\" accuracy of random forest classifier is {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936f0b1",
   "metadata": {},
   "source": [
    "## Aggregating two models on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0dde544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing Bayesian predictions on unseen data\n",
    "bayes_new_probability = bayes.predict_proba(X_bayes_new)[:,0]\n",
    "bayes_new_probability = pd.DataFrame(bayes_new_probability)\n",
    "\n",
    "# performing tree predictions on unseen data\n",
    "tree_new_probability = np.zeros((len(X_tree_new), n_tree))\n",
    "for i in range(n_tree):\n",
    "    tree = forest[i]\n",
    "    feature = features[i]\n",
    "    # transforming new data set to fit with selected features of tree\n",
    "    X_tree_new_subset = X_tree_new[feature]\n",
    "    # predict test with this tree\n",
    "    tree_new_probability[:, i] = tree.predict_proba(X_tree_new_subset)[:,0]\n",
    "# Aggregate the probabilities of all the decision trees on the new data set\n",
    "tree_new_probability = pd.DataFrame(tree_new_probability)\n",
    "tree_new_probability = tree_new_probability.mean(axis=1)\n",
    "\n",
    "# aggregating the two prediction\n",
    "new_probability = bayes_new_probability\n",
    "new_probability['tree']=tree_new_probability\n",
    "new_probability = new_probability.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13e7a483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of promos to send: 5364, with estimated profit: 212137.85547256417\n"
     ]
    }
   ],
   "source": [
    "# Predicting the yield of sending promo, based on probabilities\n",
    "yield_if_sent = (1-new_probability)*980*0.1-new_probability*310*0.05-10\n",
    "\n",
    "# Desciding whether or not to send promo, based on yield\n",
    "send_promo = (yield_if_sent > 0).astype(bool)\n",
    "send_promo = pd.Series(send_promo)\n",
    "\n",
    "# Calculating total yield of sending promo to selected people\n",
    "profit_estimate = yield_if_sent[send_promo.values].sum()\n",
    "amount_to_send = send_promo.sum()\n",
    "print(f\"amount of promos to send: {amount_to_send}, with estimated profit: {profit_estimate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "254e4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing potential customer ID's and potential profit to text file\n",
    "\n",
    "# Selecting corresponding ID's\n",
    "sending_ID = new_data_ID[send_promo.values]\n",
    "# Calculating total yield of sending promo to selected ID's\n",
    "profit_estimate = yield_if_sent[send_promo.values].sum()\n",
    "\n",
    "#writing to text file\n",
    "with open('selected_customers.txt', 'w') as f:\n",
    "    f.write(f\"Potential profit estimate: \\n {str(profit_estimate)} \\n \\n ID's of potential customers to send promo to:\")\n",
    "    f.write('\\n'.join(sending_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
