{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3664ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec6a7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data, removing redundant columns\n",
    "\n",
    "data = pd.read_csv('existing-customers.csv', sep=';', na_values=[''])\n",
    "data = data.drop(columns = ['RowID', 'education-num'])\n",
    "new_data = pd.read_csv('potential-customers.csv', sep=';', na_values=[''])\n",
    "new_data_ID = new_data['RowID']\n",
    "new_data = new_data.drop(columns = ['RowID', 'education-num'])\n",
    "\n",
    "# splitting features from label, trasforming categorical data into dummies (seperate variabe for NA)\n",
    "X = data.drop(columns = ['class'])\n",
    "X = pd.get_dummies(X, dummy_na=True)\n",
    "y = data[['class']]\n",
    "y = pd.get_dummies(y, drop_first=True)\n",
    "X_new = pd.get_dummies(new_data, dummy_na=True)\n",
    "\n",
    "# making sure new customer data has same columns as training data\n",
    "for category in X.columns.values.tolist():\n",
    "    if category not in X_new.columns:\n",
    "        X_new[category] = 0\n",
    "\n",
    "# Split the dataset into train, val, and test sets\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5)\n",
    "\n",
    "# Split the train data into multiple subsets using bootstrap sampling\n",
    "n_subsets = 20\n",
    "customer_subsets = []\n",
    "for i in range(n_subsets):\n",
    "    subset_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "    X_train_subset = X_train.iloc[subset_indices]\n",
    "    y_train_subset = y_train.iloc[subset_indices]\n",
    "    customer_subsets.append((X_train_subset, y_train_subset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0583e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions to build tree, build forest, test hyperparameters (to be used in next step)\n",
    "\n",
    "# Group the dummies from each categorical variable together (to be able to select features keeping dummies together)\n",
    "vars = ['age', 'workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country']\n",
    "groups = []\n",
    "for var in vars:\n",
    "    var_dummies = [col for col in X.columns if col.startswith(var)]\n",
    "    groups.append(var_dummies)\n",
    "\n",
    "# build a decision tree with given parameters, and predict the outcome for the validation set\n",
    "def build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, validation):\n",
    "    # randomly select a subset of features for tree\n",
    "    selected_groups = []\n",
    "    groups = random.sample(groups, k=n_feature)\n",
    "    selected_groups.append(groups)\n",
    "    selected_features = []\n",
    "    for groups in selected_groups:\n",
    "        for group in groups:\n",
    "            selected_features += group\n",
    "    # select a subset of the data for tree\n",
    "    subset = np.random.choice(len(customer_subsets), size=1)[0]\n",
    "    X_train_subset, y_train_subset = customer_subsets[subset]\n",
    "    X_train_subset = X_train_subset[selected_features]\n",
    "    # train tree\n",
    "    tree = DecisionTreeClassifier(criterion=criterion, min_samples_leaf=min_samples_leaf, max_depth=max_depth,  min_samples_split= min_samples_split)\n",
    "    tree.fit(X_train_subset, y_train_subset)\n",
    "    if validation:\n",
    "        # transforming validation set to fit with selected features of tree\n",
    "        X_val_subset = X_val[selected_features]\n",
    "        # predict val with this tree\n",
    "        predict = tree.predict(X_val_subset)\n",
    "        return predict\n",
    "    else:\n",
    "        #return tree with its selected features\n",
    "        return [tree, selected_features]\n",
    "\n",
    "# build a random forest with given parameters, and calculate its accuracy\n",
    "def build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, validation):\n",
    "    if validation:\n",
    "        val_predict = np.zeros((len(X_val), n_tree))\n",
    "        for i in range(n_tree):\n",
    "            #fill column of validation prediction for this tree\n",
    "            val_predict[:, i] = build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, True)\n",
    "        # Aggregate the predict of all the decision trees on the val set\n",
    "        val_predict = pd.DataFrame(val_predict)\n",
    "        val_predict = val_predict.mode(axis=1)[0]\n",
    "        val_acc = accuracy_score(y_val, val_predict)\n",
    "        return(val_acc)\n",
    "    else:\n",
    "        forest=[]\n",
    "        features=[]\n",
    "        for i in range(n_tree):\n",
    "            #fill column of validation prediction for this tree\n",
    "            tree, selected_features = build_tree(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, groups, False)\n",
    "            forest.append(tree) \n",
    "            features.append(selected_features)\n",
    "        return([forest, features])\n",
    "  \n",
    "    \n",
    "# test which hyperparameters are best on validation set                                \n",
    "def param_testing(n_features, criterions, min_samples_leafs, max_depths, min_samples_splits, n_trees, groups):\n",
    "    best_acc = 0\n",
    "    for n_feature in n_features:   \n",
    "        for criterion in criterions: #[\"gini\", \"entropy\", \"log_loss\"]\n",
    "            for min_samples_leaf in min_samples_leafs:\n",
    "                for max_depth in max_depths:\n",
    "                    for min_samples_split in min_samples_splits:\n",
    "                        for n_tree in n_trees:\n",
    "                            val_acc = build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, True) \n",
    "                            if val_acc > best_acc:\n",
    "                                best_params = [n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree]\n",
    "                                best_acc = val_acc\n",
    "                                print(f\"current best parameters are n_features: {best_params[0]}, criterion: {best_params[1]}, min_samples_leaf: {best_params[2]}, max_depth: {best_params[3]}, min_samples_split: {best_params[4]}, n_trees: {best_params[5]}, accuracy is {best_acc}\")\n",
    "    print(f\"\\n best parameters are n_features: {best_params[0]}, criterion: {best_params[1]}, min_samples_leaf: {best_params[2]}, max_depth: {best_params[3]}, min_samples_split: {best_params[4]}, n_trees: {best_params[5]}, accuracy is {best_acc}\")\n",
    "    return(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fc7d808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current best parameters are n_features: 9, criterion: gini, min_samples_leaf: 10, max_depth: 20, min_samples_split: 2, n_trees: 50, accuracy is 0.8597461097461098\n",
      "current best parameters are n_features: 9, criterion: gini, min_samples_leaf: 20, max_depth: 20, min_samples_split: 2, n_trees: 50, accuracy is 0.8607698607698607\n",
      "current best parameters are n_features: 9, criterion: entropy, min_samples_leaf: 10, max_depth: 20, min_samples_split: 2, n_trees: 50, accuracy is 0.8634316134316135\n",
      "\n",
      " best parameters are n_features: 9, criterion: entropy, min_samples_leaf: 10, max_depth: 20, min_samples_split: 2, n_trees: 50, accuracy is 0.8634316134316135\n"
     ]
    }
   ],
   "source": [
    "# testing which hyperparameters give best results (can be skipped: use best parameters given in next step)\n",
    "n_features = [6, 9, 12]\n",
    "criterions = [\"gini\", \"entropy\", \"log_loss\"]\n",
    "min_samples_leafs = [10, 20, 30]\n",
    "max_depths = [10, 20]\n",
    "min_samples_splits = [2]\n",
    "n_trees = [5, 10, 50]\n",
    "\n",
    "best_params = param_testing(n_features, criterions, min_samples_leafs, max_depths, min_samples_splits, n_trees, groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87041509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last found best parameters (to not have to run the testing of hyperparameters again)\n",
    "# best_params = [9, \"entropy\", 10, 20, 2, 50]\n",
    "# accuracy is 0.8634316134316135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e09d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a forest with the best found parameters\n",
    "n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree = best_params\n",
    "forest, features = build_random_forest(n_feature, criterion, min_samples_leaf, max_depth, min_samples_split, n_tree, groups, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f864cc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8552712384851586\n"
     ]
    }
   ],
   "source": [
    "# testing the acuracy of the selected random forest on test set\n",
    "\n",
    "test_predict = np.zeros((len(X_test), n_tree))\n",
    "for i in range(n_tree):\n",
    "    test_tree = forest[i]\n",
    "    test_features = features[i]\n",
    "    # transforming test set to fit with selected features of tree\n",
    "    X_test_subset = X_test[test_features]\n",
    "    # predict test with this tree\n",
    "    test_predict[:, i] = test_tree.predict(X_test_subset)\n",
    "# Aggregate the predict of all the decision trees on the val set\n",
    "test_predict = pd.DataFrame(test_predict)\n",
    "test_predict = test_predict.mode(axis=1)[0]\n",
    "test_acc = accuracy_score(y_test, test_predict)\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4cb7b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of promos to send: 6633, with estimated profit: 209897.97660882477\n"
     ]
    }
   ],
   "source": [
    "# performing predictions on unseen data, and desciding which people to send promo to\n",
    "\n",
    "new_probability = np.zeros((len(X_new), n_tree))\n",
    "for i in range(n_tree):\n",
    "    tree = forest[i]\n",
    "    feature = features[i]\n",
    "    # transforming new data set to fit with selected features of tree\n",
    "    X_new_subset = X_new[feature]\n",
    "    # predict test with this tree\n",
    "    new_probability[:, i] = tree.predict_proba(X_new_subset)[:,0]\n",
    "# Aggregate the probabilities of all the decision trees on the new data set\n",
    "new_probability = pd.DataFrame(new_probability)\n",
    "new_probability = new_probability.mean(axis=1)\n",
    "\n",
    "# Predicting the yield of sending promo, based on probabilities\n",
    "yield_if_sent = (1-new_probability)*980*0.1-new_probability*310*0.05-10\n",
    "# Desciding whether or not to send promo, based on yield\n",
    "send_promo = (yield_if_sent > 0).astype(bool)\n",
    "\n",
    "# Calculating total yield of sending promo to selected people\n",
    "profit_estimate = yield_if_sent[send_promo.values].sum()\n",
    "amount_to_send = send_promo.sum()\n",
    "print(f\"amount of promos to send: {amount_to_send}, with estimated profit: {profit_estimate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "685a0656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing potential customer ID's and potential profit to text file\n",
    "\n",
    "# Selecting corresponding ID's\n",
    "sending_ID = new_data_ID[send_promo.values]\n",
    "\n",
    "#writing to text file\n",
    "with open('selected_customers_forest.txt', 'w') as f:\n",
    "    f.write(f\"Potential profit estimate: \\n {str(profit_estimate)} \\n \\n ID's of potential customers to send promo to:\")\n",
    "    f.write('\\n'.join(sending_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
